{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import category_encoders as ce\n",
    "import math\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_sodas = pd.read_csv('C:/Users/20193222/OneDrive - TU Eindhoven/DIiA Data/data_2022_sodas.csv')\n",
    "df_2023_sodas = pd.read_csv('C:/Users/20193222/OneDrive - TU Eindhoven/DIiA Data/data_2023_sodas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_2022_sodas, df_2023_sodas], ignore_index=True)\n",
    "\n",
    "#drop Unnamed: 0 and Clouds columns\n",
    "df_all = df_all.drop(['Unnamed: 0', 'Clouds'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 10 most popular sodas\n",
    "sodas = df_all['SaleDescription'].value_counts()[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding day number (1 to 30 per month) and week number (1 to 52 weeks per year)\n",
    "df_all['Day'] = pd.to_datetime(df_all['Day'],errors ='coerce')\n",
    "df_all['Week_num'] = df_all['Day'].dt.isocalendar().week\n",
    "df_all['Day_num'] = df_all['Day'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace season names with numbers\n",
    "df_all['Season'] = df_all['Season'].replace(['Winter', 'Spring', 'Summer', 'Autumn'], [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_null(df):\n",
    "    ''' Replace null values with 'Not_holiday' and 'Not_school_holiday' '''\n",
    "    df['Holiday'].fillna('Not_holiday', inplace = True)\n",
    "    df['SchoolHoliday'].fillna('Not_school_holiday', inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_product(df, product):\n",
    "    ''' Preprocess data for a specific product in order to train a model for that product '''\n",
    "    df1 = df[df['SaleDescription'] == product]\n",
    "    dummydb = df1.groupby('Day', as_index=False).last()\n",
    "    \n",
    "    num_sales = df1.groupby(['Day'], as_index=False)['SaleDescription'].count()\n",
    "    dummydb['Num_sale'] = num_sales['SaleDescription']\n",
    "    \n",
    "    df_model = dummydb[['Weekday','Day_num', 'Week_num', 'Month', 'Year', 'Weekend', 'Season', 'Holiday', 'SchoolHoliday', 'HolidayBool', 'Avg Wind Speed', \n",
    "                        'Avg Temp', 'Min Temp', 'Max Temp', 'Sun Hours', 'Hours of rain', 'Rain','RainBool', 'Num_sale','Day']]\n",
    "    \n",
    "    \n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, column):\n",
    "    '''Standardize a column in a dataframe'''\n",
    "    df_std = df.copy()\n",
    "    df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_set(df, date):\n",
    "    ''' Split data into train and test set based on a given date '''\n",
    "    train_df = df[df['Day'] < date]\n",
    "    test_df = df[df['Day'] > date]\n",
    "\n",
    "    X_train = train_df.drop(['Num_sale','Day'], axis = 1)\n",
    "    y_train = train_df[['Num_sale']]\n",
    "    X_test = test_df.drop(['Num_sale','Day'], axis = 1)\n",
    "    y_test = test_df[['Num_sale']]\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train_test_df_for_XGBoost_lightGBM(df, date):\n",
    "    ''' Preprocess data for XGBoost and LightGBM models and split into train and test set based on a given date '''\n",
    "    # replacing all null values\n",
    "    replace_null(df)\n",
    "    \n",
    "    # Converting day_num, Month, year, weekday,Weekend, holidayBool, rainBool as categorical column\n",
    "    df['Month'] = df['Month'].map(str)\n",
    "    df['Year'] = df['Year'].map(str)\n",
    "    df['Weekday'] = df['Weekday'].map(str)\n",
    "    df['HolidayBool'] = df['HolidayBool'].map(str)\n",
    "    df['RainBool'] = df['RainBool'].map(str)\n",
    "    df['Day_num'] = df['Day_num'].map(str)\n",
    "    df['Weekend'] = df['Weekend'].map(str)\n",
    "    df['Week_num'] = df['Week_num'].map(str)\n",
    "    \n",
    "    train_df = df[df['Day'] < date]\n",
    "    test_df = df[df['Day'] > date]\n",
    "    \n",
    "    X_train = train_df.drop(['Num_sale','Day'], axis = 1)\n",
    "    y_train = train_df[['Num_sale']]\n",
    "    X_test = test_df.drop(['Num_sale','Day'], axis = 1)\n",
    "    y_test = test_df[['Num_sale']]\n",
    "    \n",
    "    # Extract text features\n",
    "    cats = list(X_train.loc[:,X_train.dtypes == 'object'].columns.values)\n",
    "\n",
    "    # Convert to Pandas category\n",
    "    for col in cats:\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "        \n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "def train_test_df_for_NN(df, date):\n",
    "    ''' Preprocess data for Neural Network model and split into train and test set based on a given date '''\n",
    "    # replacing all null values\n",
    "    replace_null(df)\n",
    "    \n",
    "    # Converting day_num, Month, year, weekday,Weekend, holidayBool, rainBool as categorical column\n",
    "    df['Month'] = df['Month'].map(str)\n",
    "    df['Year'] = df['Year'].map(str)\n",
    "    df['Weekday'] = df['Weekday'].map(str)\n",
    "    df['HolidayBool'] = df['HolidayBool'].map(str)\n",
    "    df['RainBool'] = df['RainBool'].map(str)\n",
    "    df['Day_num'] = df['Day_num'].map(str)\n",
    "    df['Weekend'] = df['Weekend'].map(str)\n",
    "    df['Week_num'] = df['Week_num'].map(str)\n",
    "    \n",
    "    train_df = df[df['Day'] < date]\n",
    "    test_df = df[df['Day'] > date]\n",
    "    \n",
    "    train_df_1 = train_df.drop(['Day'],axis = 1)\n",
    "    test_df_1 = test_df.drop(['Day'],axis = 1)\n",
    "    col_list = train_df_1.select_dtypes(include=['object']).columns.to_list()\n",
    "\n",
    "    # Encoding categorical columns\n",
    "    encoder =  ce.BinaryEncoder(cols = col_list, return_df = True )\n",
    "    train_df_encoded =  encoder.fit_transform(train_df_1)\n",
    "    test_df_encoded = encoder.transform(test_df_1)\n",
    "    \n",
    "    X_train = train_df_encoded.drop(['Num_sale'], axis = 1)\n",
    "    y_train = train_df_encoded[['Num_sale']]\n",
    "    X_test = test_df_encoded.drop(['Num_sale'], axis = 1)\n",
    "    y_test = test_df_encoded[['Num_sale']]\n",
    "    \n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, print_metrics=False):\n",
    "    ''' Evaluate model based on MAE, MSE, RMSE and R2 score '''\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    metrics = {'MAE': mean_absolute_error(y_test, y_pred),\n",
    "               'MSE': mean_squared_error(y_test, y_pred),\n",
    "               'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "               'R2': r2_score(y_test, y_pred)}\n",
    "    \n",
    "    if print_metrics:\n",
    "        print('Mean Absolute Error:', metrics['MAE'])  \n",
    "        print('Mean Squared Error:', metrics['MSE'])  \n",
    "        print('Root Mean Squared Error:', metrics['RMSE'])\n",
    "        print('R2 score:', metrics['R2'])\n",
    "        \n",
    "    return metrics\n",
    "    \n",
    "def plot_feature_importance(model, X_train):\n",
    "    ''' Plot feature importance of a model '''\n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "    feature_importances.iloc[::-1].plot(kind='barh', figsize=(10, 6))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_prediction(model, X_test, y_test):\n",
    "    ''' Plot true and predicted values '''\n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.plot(y_test, label='True')\n",
    "    plt.plot(y_pred, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_Network(X_train,y_train,X_test,y_test):\n",
    "    ''' Train a Neural Network model '''\n",
    "    hidden_units1 = 10\n",
    "    hidden_units2 = 10\n",
    "    hidden_units3 = 10\n",
    "    learning_rate = 0.01\n",
    "    # Creating model using the Sequential in tensorflow\n",
    "    def build_model_using_sequential():\n",
    "        model = Sequential([\n",
    "        Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n",
    "        Dense(1, kernel_initializer='normal', activation='linear')\n",
    "          ])\n",
    "        return model\n",
    "    \n",
    "    # build the model\n",
    "    model_NN = build_model_using_sequential()\n",
    "    # loss function\n",
    "    msle = MeanSquaredLogarithmicError()\n",
    "    model_NN.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=learning_rate), \n",
    "    metrics=[msle]\n",
    "    )\n",
    "    # train the model\n",
    "    history = model_NN.fit(\n",
    "        X_train.values, \n",
    "        y_train.values, \n",
    "        epochs=150, \n",
    "        batch_size=16,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    return model_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best combination of features for a given model\n",
    "For each model, multiple combinations of features are tested, and the best combination is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X_train, X_test, y_train, y_test, model):\n",
    "    ''' Find best feature combination for a given model '''\n",
    "    all_features = ['Weekday','Day_num', 'Week_num', 'Month', 'HolidayBool', 'Avg Wind Speed', \n",
    "                    'Avg Temp', 'Min Temp', 'Max Temp', 'Sun Hours', 'Hours of rain', 'Rain']\n",
    "\n",
    "    # These features have been defined as optional features\n",
    "    removable_features = ['Year', 'Weekend', 'Season', 'RainBool']\n",
    "    \n",
    "    results = {}\n",
    "    models = {}\n",
    "\n",
    "    # train model for all possible feature combinations\n",
    "    for r in range(len(removable_features) + 1):\n",
    "        varying_combinations = combinations(removable_features, r)\n",
    "        for combo in varying_combinations:\n",
    "            features_combination = all_features.copy()\n",
    "            for item in list(combo):\n",
    "                features_combination.append(item)\n",
    "            X_train_temp = X_train[features_combination]\n",
    "            X_test_temp = X_test[features_combination]\n",
    "            \n",
    "            # XGBoost requires a different data format\n",
    "            if type(model) == type(xgb.XGBRegressor()):\n",
    "                model.fit(X_train_temp, y_train, eval_set=[(X_test, y_test)])\n",
    "                models[combo] = model\n",
    "            else:\n",
    "                model.fit(X_train_temp, y_train)\n",
    "                models[combo] = model\n",
    "                \n",
    "            # evaluate model\n",
    "            metrics = evaluate_model(model, X_test_temp, y_test, print_metrics=False)\n",
    "            results[combo] = metrics\n",
    "\n",
    "    #find best feature combination and its metrics based on R2 score\n",
    "    best_feature_combo = max(results, key=lambda k: results[k]['R2'])\n",
    "    best_features = all_features.copy()\n",
    "    for item in list(best_feature_combo):\n",
    "        best_features.append(item)\n",
    "    best_feature_score = results[best_feature_combo]\n",
    "    best_model = models[best_feature_combo]\n",
    "    \n",
    "    return best_features, best_feature_score, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best model per soda\n",
    "For each soda, all models are tested and the best one is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(df_model, date):\n",
    "    ''' Run all models for a given product '''\n",
    "    \n",
    "    # run linear regression, random forest, support vector regression, and decision tree\n",
    "    X_train, y_train, X_test, y_test = train_test_set(df_model, date)\n",
    "    \n",
    "    # standardize y_train and y_test - this is used for some of the models\n",
    "    std_y_train = standardize(y_train, 'Num_sale')\n",
    "    std_y_test = standardize(y_test, 'Num_sale')\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr_features, lr_results, lr_model = model_selection(X_train, X_test, y_train, y_test, lr)\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "    rf_features, rf_results, rf_model = model_selection(X_train, X_test, std_y_train, std_y_test, rf)\n",
    "    \n",
    "    svr = SVR(kernel='rbf', degree = 3, C=1000, gamma=0.01, epsilon=0.1)\n",
    "    svr_features, svr_results, svr_model = model_selection(X_train, X_test, std_y_train, std_y_test, svr)\n",
    "    \n",
    "    dt = DecisionTreeRegressor(random_state = 42)\n",
    "    dt_features, dt_results, dt_model = model_selection(X_train, X_test, y_train, y_test, dt)\n",
    "    \n",
    "    # run XGBoost and LightGBM\n",
    "    X_train2, y_train2, X_test2, y_test2 = train_test_df_for_XGBoost_lightGBM(df_model, date)\n",
    "    \n",
    "    #TODO: fix xgboost --> this model works in the 'combined_model.ipynb' file\n",
    "    # xgb_model = xgb.XGBRegressor(enable_categorical=True, tree_method= 'hist', gamma = 1, random_state = 101, learning_rate = 0.001, max_depth = 30, early_stopping_rounds = 10)\n",
    "    # xgb_features, xgb_results, xgb_model = model_selection(X_train2, X_test2, y_train2, y_test2, xgb_model)\n",
    "    \n",
    "    lgbm = lightgbm.LGBMRegressor()\n",
    "    lgbm_features, lgbm_results, lgbm_model = model_selection(X_train2, X_test2, y_train2, y_test2, lgbm)\n",
    "    \n",
    "    # run neural network\n",
    "    X_train3, y_train3, X_test3, y_test3 = train_test_df_for_NN(df_model, date)\n",
    "    \n",
    "    #TODO: fix neural network --> this model works in the 'combined_model.ipynb' file\n",
    "    # NN_model = Neural_Network(X_train3,y_train3,X_test3,y_test3)\n",
    "    # nn_features, nn_results, nn_model = model_selection(X_train3, X_test3, y_train3, y_test3, NN_model)\n",
    "    \n",
    "    # save results\n",
    "    metrics = {'Linear Regression': lr_results,\n",
    "                'Random Forest': rf_results,\n",
    "                'Support Vector Regression': svr_results,\n",
    "                'Decision Tree': dt_results,\n",
    "                # 'XGBoost': xgb_results,\n",
    "                'LightGBM': lgbm_results,\n",
    "                # 'Neural Network': nn_results,\n",
    "                }\n",
    "    \n",
    "    features = {'Linear Regression': lr_features,\n",
    "                'Random Forest': rf_features,\n",
    "                'Support Vector Regression': svr_features,\n",
    "                'Decision Tree': dt_features,\n",
    "                # 'XGBoost': xgb_features,\n",
    "                'LightGBM': lgbm_features,\n",
    "                # 'Neural Network': nn_features,\n",
    "                }\n",
    "    \n",
    "    models = {'Linear Regression': lr_model,\n",
    "                'Random Forest': rf_model,\n",
    "                'Support Vector Regression': svr_model,\n",
    "                'Decision Tree': dt_model,\n",
    "                # 'XGBoost': xgb_model,\n",
    "                'LightGBM': lgbm_model,\n",
    "                # 'Neural Network': nn_model,\n",
    "                }\n",
    "    \n",
    "    # find best model\n",
    "    best_model = max(metrics, key=lambda k: metrics[k]['R2'])\n",
    "    best_model_score = metrics[best_model]\n",
    "    best_model_features = features[best_model]\n",
    "    \n",
    "    return metrics, features, models, best_model, best_model_score, best_model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline\n",
    "This pipeline runs for each product (soda) in the data, and includes preprocessing, running the models and feature selection steps. It also makes predictions using the best model for each soda and saves them to a xlsx file per soda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "\n",
    "for soda in sodas:\n",
    "    df_predictions = pd.DataFrame(columns=['Day', 'Num_sale', 'Predicted_sales'])\n",
    "    df_model = preprocess_product(df_all, soda)\n",
    "    metrics, features, models, best_model, best_model_score, best_model_features = run_models(df_model, '2023-05-30')\n",
    "    \n",
    "    print(f'Best model for {soda} is: {best_model}')\n",
    "    metrics_dict[soda] = metrics\n",
    "    \n",
    "    #get predictions with best_model\n",
    "    model = models[best_model]\n",
    "    days = list(df_model[df_model['Day'] > '2023-05-30']['Day'])\n",
    "    \n",
    "    if best_model == 'LightGBM':\n",
    "        X_train, y_train, X_test, y_test = train_test_df_for_XGBoost_lightGBM(df_model, '2023-05-30')\n",
    "    elif best_model == 'Neural Network':\n",
    "        X_train, y_train, X_test, y_test = train_test_df_for_NN(df_model, '2023-05-30')\n",
    "    else: \n",
    "        X_train, y_train, X_test, y_test = train_test_set(df_model, '2023-05-30')\n",
    "        \n",
    "    pred_data = X_test\n",
    "    pred_features = pred_data[['Weekday','Day_num', 'Week_num', 'Month', 'HolidayBool', 'Avg Wind Speed', \n",
    "            'Avg Temp', 'Min Temp', 'Max Temp', 'Sun Hours', 'Hours of rain', 'Rain', \n",
    "            'Year', 'Weekend', 'Season', 'RainBool']]\n",
    "    pred = model.predict(pred_features)\n",
    "    \n",
    "    \n",
    "    df_predictions['Num_sale'] = y_test\n",
    "    df_predictions['Day'] = days\n",
    "    df_predictions['predict'] = [int(x) for x in pred]\n",
    "    df_predictions.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # dynamically allocating file name\n",
    "    filename = f'{soda}.xlsx'\n",
    "\n",
    "    df_predictions.to_excel(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only R2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the model performance for the models tested in the combined_model.ipynb file\n",
    "model_performance = pd.read_csv('model_performance.csv')\n",
    "model_performance_small = model_performance.copy()\n",
    "model_performance_small['R2'] = model_performance_small['R2_score'].round(2)\n",
    "model_performance_small = model_performance_small.drop(columns=['MSE','RMSE','MAE', 'R2_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get highest R2 score for each soda\n",
    "model_performance_small = model_performance_small.sort_values(by=['R2'], ascending=False)\n",
    "model_performance_small = model_performance_small.drop_duplicates(subset=['Product_Name'], keep='first')\n",
    "model_performance_small = model_performance_small.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model for each soda\n",
    "r2_scores = {}\n",
    "for soda in sodas:\n",
    "    r2_scores[soda] = {}\n",
    "    for model in metrics_dict[soda]:\n",
    "        if 'R2' in metrics_dict[soda][model]:\n",
    "            r2_scores[soda][model] = metrics_dict[soda][model]['R2']\n",
    "\n",
    "r2_scores_df = pd.DataFrame(r2_scores)\n",
    "#pivot table\n",
    "r2_scores_df = r2_scores_df.transpose()\n",
    "r2_scores_df['Best model'] = r2_scores_df.idxmax(axis=1)\n",
    "\n",
    "r2_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores = r2_scores_df.reset_index()\n",
    "r2_scores = r2_scores.rename(columns={'index': 'Product_Name'})\n",
    "\n",
    "#get max r2 score for each soda\n",
    "r2_scores['R2_score'] = r2_scores[['Linear Regression', 'Decision Tree']].max(axis=1).round(2)\n",
    "r2_scores = r2_scores.drop(columns=['Linear Regression', 'Decision Tree'])\n",
    "\n",
    "#merge with model_performance\n",
    "merged_scores = pd.merge(model_performance_small, r2_scores, on='Product_Name')\n",
    "merged_scores\n",
    "\n",
    "df_r2_scores = pd.DataFrame(columns=['Product_Name', 'Model', 'R2_score'])\n",
    "for i in range(len(merged_scores)):\n",
    "    if merged_scores['R2'][i] > merged_scores['R2_score'][i]:\n",
    "        df_r2_scores.loc[i] = [merged_scores['Product_Name'][i], merged_scores['Model_name'][i], merged_scores['R2'][i]]\n",
    "    else:\n",
    "        df_r2_scores.loc[i] = [merged_scores['Product_Name'][i], merged_scores['Best model'][i], merged_scores['R2_score'][i]]\n",
    "        \n",
    "df_r2_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_df = pd.DataFrame(metrics_dict)\n",
    "#pivot table\n",
    "metrics_dict_df = metrics_dict_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe per soda with all model metrics\n",
    "df_metrics = pd.DataFrame(columns=['Product_Name', 'Model_name', 'MAE', 'MSE', 'RMSE', 'R2_score'])\n",
    "for soda in sodas:\n",
    "    df_metrics = pd.concat([df_metrics, model_performance[model_performance['Product_Name'] == soda]], ignore_index=True)\n",
    "    #also add models from metrics_dict\n",
    "    for i in range(0, len(metrics_dict_df)):\n",
    "        if metrics_dict_df.index[i] == soda:\n",
    "            for model in metrics_dict_df.columns:\n",
    "                df_model = pd.DataFrame(metrics_dict_df.loc[soda, model], index=[0])\n",
    "                df_model['Product_Name'] = soda\n",
    "                df_model['Model_name'] = model\n",
    "                df_model.rename(columns={'MAE': 'MAE', 'MSE': 'MSE', 'RMSE': 'RMSE', 'R2': 'R2_score'}, inplace=True)\n",
    "                df_metrics = pd.concat([df_metrics, df_model], ignore_index=True)\n",
    "#round everything to 2 decimals\n",
    "df_metrics = df_metrics.round(2)\n",
    "\n",
    "#split dataframe into dataframe per product_name\n",
    "for soda in sodas:\n",
    "    df_metrics[df_metrics['Product_Name'] == soda].to_csv(f'{soda}_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soda = 'FANTA EXOTIC 330ML' #change to soda of your choice\n",
    "metrics_dict = {}\n",
    "df_predictions = pd.DataFrame(columns=['Day', 'SaleDescription', 'Num_sale', 'predict'])\n",
    "\n",
    "df_model = preprocess_product(df_all, soda)\n",
    "X_train_imp, y_train_imp, X_test_imp, y_test_imp = train_test_df_for_XGBoost_lightGBM(df_model, '2023-05-30') #change to data and train/test split of your choice\n",
    "\n",
    "lgbm = lightgbm.LGBMRegressor() #change to model of your choice\n",
    "lgbm.fit(X_train_imp, y_train_imp, eval_set=[(X_test_imp, y_test_imp)])\n",
    "plot_feature_importance(lgbm, X_train_imp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
